{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import imblearn\n",
    "import sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.multiclass import type_of_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the print content to file \n",
    "f1 = open(\"2-2-5 IoT predictive scores for an static model by target names.log\",'w+')\n",
    "f2 = open(\"2-2-5 IoT predictive scores for an static model with cv.log\",'w+')\n",
    "f3 = open(\"2-2-5 IoT predictive scores for an static model on kafka data with cv.log\",'w+')\n",
    "f4 = open(\"2-2-5 IoT predictive scores for an static model on kafka data by target names.log\",'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_infinity(X):\n",
    "    # check for values approaching infinity\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"the number of infinity:\", X.isna().sum().sum())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_datatypes = {\n",
    "    'MI_dir_L5_weight': np.float64,\n",
    "    'MI_dir_L5_mean': np.float64,\n",
    "    'MI_dir_L5_variance': np.float64,\n",
    "    'MI_dir_L3_weight': np.float64,\n",
    "    'MI_dir_L3_mean': np.float64,\n",
    "    'MI_dir_L3_variance': np.float64,\n",
    "    'MI_dir_L1_weight': np.float64,\n",
    "    'MI_dir_L1_mean': np.float64,\n",
    "    'MI_dir_L1_variance': np.float64,\n",
    "    'MI_dir_L0.1_weight': np.float64,\n",
    "    'MI_dir_L0.1_mean': np.float64,\n",
    "    'MI_dir_L0.1_variance': np.float64,\n",
    "    'MI_dir_L0.01_weight': np.float64,\n",
    "    'MI_dir_L0.01_mean': np.float64,\n",
    "    'MI_dir_L0.01_variance': np.float64,\n",
    "    'H_L5_weight': np.float64,\n",
    "    'H_L5_mean': np.float64,\n",
    "    'H_L5_variance': np.float64,\n",
    "    'H_L3_weight': np.float64,\n",
    "    'H_L3_mean': np.float64,\n",
    "    'H_L3_variance': np.float64,\n",
    "    'H_L1_weight': np.float64,\n",
    "    'H_L1_mean': np.float64,\n",
    "    'H_L1_variance': np.float64,\n",
    "    'H_L0.1_weight': np.float64,\n",
    "    'H_L0.1_mean': np.float64,\n",
    "    'H_L0.1_variance': np.float64,\n",
    "    'H_L0.01_weight': np.float64,\n",
    "    'H_L0.01_mean': np.float64,\n",
    "    'H_L0.01_variance': np.float64,\n",
    "    'HH_L5_weight': np.float64,\n",
    "    'HH_L5_mean': np.float64,\n",
    "    'HH_L5_std': np.float64,\n",
    "    'HH_L5_magnitude': np.float64,\n",
    "    'HH_L5_radius': np.float64,\n",
    "    'HH_L5_covariance': np.float64,\n",
    "    'HH_L5_pcc': np.float64,\n",
    "    'HH_L3_weight': np.float64,\n",
    "    'HH_L3_mean': np.float64,\n",
    "    'HH_L3_std': np.float64,\n",
    "    'HH_L3_magnitude': np.float64,\n",
    "    'HH_L3_radius': np.float64,\n",
    "    'HH_L3_covariance': np.float64,\n",
    "    'HH_L3_pcc': np.float64,\n",
    "    'HH_L1_weight': np.float64,\n",
    "    'HH_L1_mean': np.float64,\n",
    "    'HH_L1_std': np.float64,\n",
    "    'HH_L1_magnitude': np.float64,\n",
    "    'HH_L1_radius': np.float64,\n",
    "    'HH_L1_covariance': np.float64,\n",
    "    'HH_L1_pcc': np.float64,\n",
    "    'HH_L0.1_weight': np.float64,\n",
    "    'HH_L0.1_mean': np.float64,\n",
    "    'HH_L0.1_std': np.float64,\n",
    "    'HH_L0.1_magnitude': np.float64,\n",
    "    'HH_L0.1_radius': np.float64,\n",
    "    'HH_L0.1_covariance': np.float64,\n",
    "    'HH_L0.1_pcc': np.float64,\n",
    "    'HH_L0.01_weight': np.float64,\n",
    "    'HH_L0.01_mean': np.float64,\n",
    "    'HH_L0.01_std': np.float64,\n",
    "    'HH_L0.01_magnitude': np.float64,\n",
    "    'HH_L0.01_radius': np.float64,\n",
    "    'HH_L0.01_covariance': np.float64,\n",
    "    'HH_L0.01_pcc': np.float64,\n",
    "    'HH_jit_L5_weight': np.float64,\n",
    "    'HH_jit_L5_mean': np.float64,\n",
    "    'HH_jit_L5_variance': np.float64,\n",
    "    'HH_jit_L3_weight': np.float64,\n",
    "    'HH_jit_L3_mean': np.float64,\n",
    "    'HH_jit_L3_variance': np.float64,\n",
    "    'HH_jit_L1_weight': np.float64,\n",
    "    'HH_jit_L1_mean': np.float64,\n",
    "    'HH_jit_L1_variance': np.float64,\n",
    "    'HH_jit_L0.1_weight': np.float64,\n",
    "    'HH_jit_L0.1_mean': np.float64,\n",
    "    'HH_jit_L0.1_variance': np.float64,\n",
    "    'HH_jit_L0.01_weight': np.float64,\n",
    "    'HH_jit_L0.01_mean': np.float64,\n",
    "    'HH_jit_L0.01_variance': np.float64,\n",
    "    'HpHp_L5_weight': np.float64,\n",
    "    'HpHp_L5_mean': np.float64,\n",
    "    'HpHp_L5_std': np.float64,\n",
    "    'HpHp_L5_magnitude': np.float64,\n",
    "    'HpHp_L5_radius': np.float64,\n",
    "    'HpHp_L5_covariance': np.float64,\n",
    "    'HpHp_L5_pcc': np.float64,\n",
    "    'HpHp_L3_weight': np.float64,\n",
    "    'HpHp_L3_mean': np.float64,\n",
    "    'HpHp_L3_std': np.float64,\n",
    "    'HpHp_L3_magnitude': np.float64,\n",
    "    'HpHp_L3_radius': np.float64,\n",
    "    'HpHp_L3_covariance': np.float64,\n",
    "    'HpHp_L3_pcc': np.float64,\n",
    "    'HpHp_L1_weight': np.float64,\n",
    "    'HpHp_L1_mean': np.float64,\n",
    "    'HpHp_L1_std': np.float64,\n",
    "    'HpHp_L1_magnitude': np.float64,\n",
    "    'HpHp_L1_radius': np.float64,\n",
    "    'HpHp_L1_covariance': np.float64,\n",
    "    'HpHp_L1_pcc': np.float64,\n",
    "    'HpHp_L0.1_weight': np.float64,\n",
    "    'HpHp_L0.1_mean': np.float64,\n",
    "    'HpHp_L0.1_std': np.float64,\n",
    "    'HpHp_L0.1_magnitude': np.float64,\n",
    "    'HpHp_L0.1_radius': np.float64,\n",
    "    'HpHp_L0.1_covariance': np.float64,\n",
    "    'HpHp_L0.1_pcc': np.float64,\n",
    "    'HpHp_L0.01_weight': np.float64,\n",
    "    'HpHp_L0.01_mean': np.float64,\n",
    "    'HpHp_L0.01_std': np.float64,\n",
    "    'HpHp_L0.01_magnitude': np.float64,\n",
    "    'HpHp_L0.01_radius': np.float64,\n",
    "    'HpHp_L0.01_covariance': np.float64,\n",
    "    'HpHp_L0.01_pcc': np.float64,\n",
    "    #'Source':object,\n",
    "    'x0_0': np.float64,\n",
    "    'x0_1': np.float64,\n",
    "    'x0_2': np.float64,\n",
    "    'x0_3': np.float64,\n",
    "    'x0_4': np.float64,\n",
    "    'x0_5': np.float64,\n",
    "    'x0_6': np.float64,\n",
    "    'x0_7': np.float64,\n",
    "    'x0_8': np.float64,\n",
    "    #'Class':object\n",
    "}\n",
    "used_cols = (ids_datatypes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from dataset (after feature extraction )\n",
    "# train model: aftcat, CV(Pipeline(imbalanced, MinMaxScaler, selector, model))\n",
    "data_aftcat = pd.read_csv(r\"./2-2-1 5388 data_aftcat.csv\")\n",
    "data_aftcat_y = pd.read_csv(r\"./2-2-1 5388 data_aftcat class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_kafka = pd.read_csv('./2-2-4 5388 data_kafka_2.csv',dtype=ids_datatypes, usecols=used_cols, low_memory=False)\n",
    "testdata_kafka_label = pd.read_csv(r\"./2-2-4 5388 data_kafka_2 label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of infinity: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: drop infinity\n",
    "testdata_kafka = drop_infinity(testdata_kafka)\n",
    "#data = drop_na(testdata_kafka)\n",
    "#print(\"after drop infinity:\", testdata_kafka.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- transform all feature with MinMaxScaler()---------------- \n",
    "mm = StandardScaler().fit(data_aftcat)\n",
    "\n",
    "data_aftcat_aftmm = pd.DataFrame(mm.transform(data_aftcat))\n",
    "data_aftcat_aftmm.columns = mm.get_feature_names_out()\n",
    "\n",
    "testdata_kafka_aftmm = pd.DataFrame(mm.transform(testdata_kafka))\n",
    "testdata_kafka_aftmm.columns = mm.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^ train models: RF, GRB, MLP, DT ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# repeat several times for parameters tuning\n",
    "# author:           Kun Yan\n",
    "# student number:   300259303\n",
    "# data:             2021-10-03\n",
    "# Python version:   3.9.7\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# ---------------- Split train datasets and test ---------------\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_aftcat_aftmm, data_aftcat_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- train model and get score by attack types ---------------\n",
    "#mm = MinMaxScaler()\n",
    "smo = SMOTE()\n",
    "selector = VarianceThreshold(np.median(X_train.var().values))\n",
    "# Random Forest model by parameters tuning\n",
    "model_rf = RandomForestClassifier(criterion='entropy',min_samples_split=2, min_samples_leaf=1, max_features=17, max_depth=23, n_estimators=115,n_jobs=-1,random_state = 90)\n",
    "#model_grb = GradientBoostingClassifier(max_features=3,learning_rate=0.1,n_estimators=130,min_samples_split=100,min_samples_leaf=7,max_depth=15,random_state = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('smo', SMOTE()),\n",
       "                ('selector', VarianceThreshold(threshold=0.9987156517276344)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(criterion='entropy', max_depth=23,\n",
       "                                        max_features=17, n_estimators=115,\n",
       "                                        n_jobs=-1, random_state=90))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_steps = [('smo',smo),('selector',selector),('model',model_rf)]\n",
    "mul_pipe = imblearn.pipeline.Pipeline(steps=pipe_steps)\n",
    "\n",
    "mul_pipe.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = prediction score for static model by target names\n",
      "{'BENIGN': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 6639}, 'mirai_udp_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 4}, 'mirai_ack_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 104}, 'gafgyt_scan_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 53}, 'mirai_scan_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 51}, 'gafgyt_tcp_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 126}, 'gafgyt_udp_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 30}, 'gafgyt_junk_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 26}, 'gafgyt_combo_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 28}, 'mirai_syn_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 522}, 'mirai_udpplain_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 7586}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 7586}}\n"
     ]
    }
   ],
   "source": [
    "mul_y_pred = mul_pipe.predict(X_test)\n",
    "#mul_pipe.score(X_test, y_test)\n",
    "mul_score = classification_report(y_test,mul_y_pred,target_names=['BENIGN','mirai_udp_attack', 'mirai_ack_attack', \n",
    "    'gafgyt_scan_attack', 'mirai_scan_attack', 'gafgyt_tcp_attack',\n",
    "    'gafgyt_udp_attack', 'gafgyt_junk_attack', 'gafgyt_combo_attack',\n",
    "    'mirai_syn_attack', 'mirai_udpplain_attack'],output_dict=True)\n",
    "print(\"= = = = = = = = = = prediction score for static model by target names\")\n",
    "print(\"= = = = = = = = = = prediction score for static model by target names\",file=f1)\n",
    "print(mul_score,file=f1)\n",
    "print(mul_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model with cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^ train models using CV ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# repeat several times for parameters tuning\n",
    "# author:           Kun Yan\n",
    "# student number:   300259303\n",
    "# data:             2021-10-03\n",
    "# Python version:   3.9.7\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "def train_model(sm, se, model, X, y):\n",
    "    #print(model, \"\\n\")\n",
    "    #pipe_steps = [('smo',sm),('mm',m),('selector',se),('model',model)]\n",
    "    pipe_steps = [('smo',sm),('selector',se),('model',model)]\n",
    "    id_pipeline = imblearn.pipeline.Pipeline(steps=pipe_steps)\n",
    "\n",
    "    \n",
    "    # evaluate the pipeline using the crossvalidation technique defined in cv\n",
    "\n",
    "    # ------------------- score ------------------------------------\n",
    "    \n",
    "    scoring = {'f1_micro', 'precision_micro', 'balanced_accuracy','recall_micro'}\n",
    "    score_sf = cross_validate(id_pipeline, X, y.values.ravel(),cv=10,scoring=scoring, n_jobs=-1)\n",
    "    #sorted(score_sf.keys())\n",
    "    #score_sf_re = score_mean_std(pd.DataFrame(score_sf))\n",
    "    return score_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- train predictive model with pipeline: SMOTE(),VarianceThreshold(), cross_validate()--------\n",
    "#mm = MinMaxScaler()\n",
    "smo = SMOTE()\n",
    "selector = VarianceThreshold(np.median(data_aftcat_aftmm.var().values))\n",
    "# Random Forest model by parameters tuning\n",
    "model_rf = RandomForestClassifier(criterion='entropy',min_samples_split=2, min_samples_leaf=1, max_features=17, max_depth=23, n_estimators=115,n_jobs=-1,random_state = 90)\n",
    "#model_grb = GradientBoostingClassifier(max_features=3,learning_rate=0.1,n_estimators=130,min_samples_split=100,min_samples_leaf=7,max_depth=15,random_state = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = = = = = = prediction score for a static model RF with CV = = = = = \n",
      "RandomForestClassifier(criterion='entropy', max_depth=23, max_features=17,\n",
      "                       n_estimators=115, n_jobs=-1, random_state=90)\n",
      "{'fit_time': array([261.41063595, 263.53414512, 263.64361501, 260.66043496,\n",
      "       257.13157296, 254.11399722, 255.23360205, 256.5883882 ,\n",
      "       132.99567604, 131.89912009]), 'score_time': array([0.08044219, 0.11874795, 0.11082697, 0.07692003, 0.07893705,\n",
      "       0.07331371, 0.09712911, 0.08731198, 0.07222176, 0.06180501]), 'test_recall_micro': array([0.99960459, 1.        , 0.99960459, 0.99960459, 0.99960443,\n",
      "       0.99920886, 0.99920886, 0.99920886, 1.        , 0.99960443]), 'test_f1_micro': array([0.99960459, 1.        , 0.99960459, 0.99960459, 0.99960443,\n",
      "       0.99920886, 0.99920886, 0.99920886, 1.        , 0.99960443]), 'test_precision_micro': array([0.99960459, 1.        , 0.99960459, 0.99960459, 0.99960443,\n",
      "       0.99920886, 0.99920886, 0.99920886, 1.        , 0.99960443]), 'test_balanced_accuracy': array([0.95454545, 1.        , 0.99760766, 0.98989899, 0.99948927,\n",
      "       0.98744199, 0.90909091, 0.90909091, 1.        , 0.95454545])}\n"
     ]
    }
   ],
   "source": [
    "# -------------------- model: RF (repeat several times for parameters tuning)-----------------------\n",
    "print(\"= = = = = = = = = = = = = = prediction score for a static model RF with CV = = = = = \")\n",
    "print(\"= = = = = = = = = = = = = = prediction score for a static model RF with CV = = = = = \",file=f2)\n",
    "print(model_rf)\n",
    "print(model_rf,file=f2)\n",
    "\n",
    "#score_sf_rf,score_sf_re_rf = train_model(smo, selector, model_rf, data_aftcat_aftmm, data_aftcat_y)\n",
    "score_sf_rf = train_model(smo, selector, model_rf, data_aftcat_aftmm, data_aftcat_y)\n",
    "\n",
    "print(score_sf_rf)\n",
    "print(score_sf_rf,file=f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = = = prediction score for kafka data = = = = = = = =\n",
      "{'fit_time': array([202.76798391, 191.77870178, 179.03235507, 193.20384288,\n",
      "       167.80275702, 157.58115101, 144.62086105, 149.98726892,\n",
      "        83.72340894,  77.20401812]), 'score_time': array([0.13840199, 0.16613913, 0.18430591, 0.14023304, 0.18555593,\n",
      "       0.17454219, 0.13685918, 0.13795304, 0.13421106, 0.12044597]), 'test_recall_micro': array([0.99070093, 0.99110089, 0.99080092, 0.9911    , 0.991     ,\n",
      "       0.9997    , 0.9909    , 0.9903    , 0.9905    , 0.9906    ]), 'test_f1_micro': array([0.99070093, 0.99110089, 0.99080092, 0.9911    , 0.991     ,\n",
      "       0.9997    , 0.9909    , 0.9903    , 0.9905    , 0.9906    ]), 'test_precision_micro': array([0.99070093, 0.99110089, 0.99080092, 0.9911    , 0.991     ,\n",
      "       0.9997    , 0.9909    , 0.9903    , 0.9905    , 0.9906    ]), 'test_balanced_accuracy': array([0.84936742, 0.90908045, 0.88358789, 0.90791495, 0.90779441,\n",
      "       0.99775865, 0.90573998, 0.81938479, 0.84979415, 0.83812288])}\n"
     ]
    }
   ],
   "source": [
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^ prediction with kafka data +cv ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# repeat several times for parameters tuning\n",
    "# author:           Kun Yan\n",
    "# student number:   300259303\n",
    "# data:             2021-10-03\n",
    "# Python version:   3.9.7\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# -------------- prediction with pipeline: SMOTE(),VarianceThreshold(), cross_validate()--------\n",
    "def pred_kafka(se, model, X, y):\n",
    "    #print(model, \"\\n\")\n",
    "    #pipe_steps = [('mm',m),('selector',se),('model',model)]\n",
    "    pipe_steps = [('selector',se),('model',model)]\n",
    "    id_pipeline = sklearn.pipeline.Pipeline(steps=pipe_steps)\n",
    "\n",
    "    # evaluate the pipeline using the crossvalidation technique defined in cv\n",
    "\n",
    "    # ------------------- score ------------------------------------\n",
    "    scoring = {'f1_micro', 'precision_micro', 'balanced_accuracy','recall_micro'}\n",
    "    score_sf = cross_validate(id_pipeline, X, y.values.ravel(),cv=10,scoring=scoring, n_jobs=-1)\n",
    "    return score_sf\n",
    "\n",
    "\n",
    "print(\"= = = = = = = = = prediction score for kafka data = = = = = = = =\")\n",
    "print(\"= = = = = = = = = prediction score for kafka data = = = = = = = =\",file=f3)\n",
    "pred_score_sf_kafka_rf = pred_kafka(selector, model_rf, testdata_kafka_aftmm, testdata_kafka_label)\n",
    "print(pred_score_sf_kafka_rf)\n",
    "print(pred_score_sf_kafka_rf,file=f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = = = =  prediction score for kafka data by target names = = = = =  \n",
      "{'BENIGN': {'precision': 0.9998619150086878, 'recall': 0.9999884914606638, 'f1-score': 0.9999251992289767, 'support': 86892}, 'mirai_udp_attack': {'precision': 0.95, 'recall': 0.5135135135135135, 'f1-score': 0.6666666666666667, 'support': 37}, 'mirai_ack_attack': {'precision': 0.9885568976478067, 'recall': 0.9987154784842646, 'f1-score': 0.9936102236421726, 'support': 1557}, 'gafgyt_scan_attack': {'precision': 1.0, 'recall': 0.9974424552429667, 'f1-score': 0.998719590268886, 'support': 782}, 'mirai_scan_attack': {'precision': 0.9988518943742825, 'recall': 0.9965635738831615, 'f1-score': 0.9977064220183487, 'support': 873}, 'gafgyt_tcp_attack': {'precision': 0.9994089834515366, 'recall': 0.9988186650915535, 'f1-score': 0.9991137370753324, 'support': 1693}, 'gafgyt_udp_attack': {'precision': 1.0, 'recall': 0.996996996996997, 'f1-score': 0.9984962406015038, 'support': 333}, 'gafgyt_junk_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 411}, 'gafgyt_combo_attack': {'precision': 1.0, 'recall': 0.997624703087886, 'f1-score': 0.9988109393579072, 'support': 421}, 'mirai_syn_attack': {'precision': 0.9998557206752272, 'recall': 0.9994231323911162, 'f1-score': 0.999639379733141, 'support': 6934}, 'mirai_udpplain_attack': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 70}, 'accuracy': 0.999660010199694, 'macro avg': {'precision': 0.994230491923413, 'recall': 0.9544624554683749, 'f1-score': 0.9684262180539033, 'support': 100003}, 'weighted avg': {'precision': 0.999653323285369, 'recall': 0.999660010199694, 'f1-score': 0.9996321339542031, 'support': 100003}}\n"
     ]
    }
   ],
   "source": [
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^ prediction with kafka by target names ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# repeat several times for parameters tuning\n",
    "# author:           Kun Yan\n",
    "# student number:   300259303\n",
    "# data:             2021-10-30\n",
    "# Python version:   3.9.7\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#target_names = ['class 0', 'class 1', 'class 2']\n",
    "#print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "print(\"= = = = = = = =  prediction score for kafka data by target names = = = = =  \")\n",
    "print(\"= = = = = = = =  prediction score for kafka data by target names = = = = =  \",file=f4)\n",
    "\n",
    "\n",
    "#pre_pipe_steps = [('selector',selector),('model',model_rf)]\n",
    "pre_pipe_steps = [('model',model_rf)]\n",
    "pre_id_pipeline = sklearn.pipeline.Pipeline(steps=pre_pipe_steps)\n",
    "pre_id_pipeline.fit(data_aftcat_aftmm, data_aftcat_y.values.ravel())\n",
    "pred_kafka_y = pre_id_pipeline.predict(testdata_kafka_aftmm)\n",
    "\n",
    "mul_score_kafka = classification_report(testdata_kafka_label,pred_kafka_y,target_names=['BENIGN','mirai_udp_attack', 'mirai_ack_attack', \n",
    "    'gafgyt_scan_attack', 'mirai_scan_attack', 'gafgyt_tcp_attack',\n",
    "    'gafgyt_udp_attack', 'gafgyt_junk_attack', 'gafgyt_combo_attack',\n",
    "    'mirai_syn_attack', 'mirai_udpplain_attack'],output_dict=True)\n",
    "print(mul_score_kafka,file=f4)\n",
    "print(mul_score_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c701a3eaa56c792080692be7ed804fc7337106df51979399b9629b6524716ca"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
